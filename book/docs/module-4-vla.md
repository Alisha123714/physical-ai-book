---
id: module-4-vla
title: Module 4 — Vision-Language-Action
sidebar_label: VLA
---

# Module 4 — Vision-Language-Action (VLA)

<div style={{ display: "flex", gap: "10px", marginBottom: "20px" }}>
  <button onClick={() => window.translateToUrdu()} style={{ padding: "10px", cursor: "pointer" }}>
    Urdu Translation
  </button>
  
  <button onClick={() => window.personalizeContent()} style={{ padding: "10px", cursor: "pointer" }}>
    Personalize Content
  </button>
</div>


## Learning Outcomes
- Convert **voice commands** to actions using OpenAI Whisper.
- Plan robot tasks via **LLMs**.
- Integrate **vision, language, and action** pipelines.

## Module Overview
This module connects **AI reasoning** with real-world action:

1. Receive voice commands like “Clean the room”.
2. Plan sequences of ROS 2 actions.
3. Recognize objects with computer vision.
4. Execute manipulation tasks on humanoid robots.

## Key Tools
- **OpenAI Whisper**
- **GPT/LLMs**
- **ROS 2 Actions**
- **Computer Vision APIs**

## Placeholder Image
<img src="/images/module4-vla.png" alt="Vision Language Action" style={{ width: "100%", height: "auto", marginTop: "20px" }} />
